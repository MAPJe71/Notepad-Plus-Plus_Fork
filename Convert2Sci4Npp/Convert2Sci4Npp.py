#!/usr/bin/env python

#-=============================================================================-
#   Convert Scintilla to Scintilla-for-Notepad++
#-=============================================================================-

from __future__ import with_statement

import argparse, codecs, glob, inspect, os, re, shutil, string, sys
sys.dont_write_bytecode = True

#-=============================================================================-

lineEnd       = "\r\n" if sys.platform == "win32" else "\n"
scriptName    = os.path.basename( sys.argv[0] ).split( '.' )[0]
scriptVersion = 1.0

#-------------------------------------------------------------------------------

class CommentNpp:
    _defaultSymbol = '//'

    def __init__( self ):
        self.name   = 'Notepad++'

    def Prefix( self, symbol = _defaultSymbol ):
        return '%s [%s]' % ( symbol, self.name )

    def AutoUpdated( self, symbol = _defaultSymbol ):
        return ' %s**: Autoupdated by %s'     % ( self.Prefix( symbol )
                                                , scriptName )

    def AutoUpdatedBegin( self, symbol = _defaultSymbol ):
        return  '%s*+: Autoupdated by %s%s'   % ( self.Prefix( symbol )
                                                , scriptName
                                                , lineEnd )

    def AutoUpdatedEnd( self, symbol = _defaultSymbol ):
        return  '%s*-: Autoupdated by %s%s'   % ( self.Prefix( symbol )
                                                , scriptName
                                                , lineEnd )

    def AutoGenerated( self, symbol = _defaultSymbol ):
        return ' %s~~: Autogenerated by %s'   % ( self.Prefix( symbol )
                                                , scriptName )

    def AutoGeneratedBegin( self, symbol = _defaultSymbol ):
        return  '%s~+: Autogenerated by %s%s' % ( self.Prefix( symbol )
                                                , scriptName
                                                , lineEnd )

    def AutoGeneratedEnd( self, symbol = _defaultSymbol ):
        return  '%s~-: Autogenerated by %s%s' % ( self.Prefix( symbol )
                                                , scriptName
                                                , lineEnd )

NppComment = CommentNpp()

#-------------------------------------------------------------------------------

def EnsureTrailingBackslash( path ):
    if path == None:
        return None
    return path if path[-1:] == "\\" else path + "\\"

#-------------------------------------------------------------------------------

def CopyFiles( srcGlob, dstFolder ):
    for path in glob.iglob( srcGlob ):
        shutil.copy( path
                   , os.path.join( dstFolder, os.path.basename( path ) ) )

#-------------------------------------------------------------------------------

def ReadFile( path ):
    try:
        with codecs.open( path, "r", "UTF-8" ) as fileIn:
            dataRead = fileIn.read()
    except IOError:
        print( "Can not open %s" % path )
        dataRead = None
    return dataRead

#-------------------------------------------------------------------------------

def UpdateFile( path, dataUpdated ):
    """
        If the file contents are different to dataUpdated then copy dataUpdated
        into the file else leave it alone so it isn't treated as modified.
    """
    fileState = "Changed"
    try:
        with codecs.open( path, "r", "utf-8" ) as fileIn:
            dataOriginal = fileIn.read()
        if dataUpdated == dataOriginal:
            # Same as before so don't write
            return
        os.unlink( path )
    except IOError: # File is not there yet
        fileState = "New"
    with codecs.open( path, "w", "utf-8" ) as fileOut:
        fileOut.write( dataUpdated )
    print( "%s %s" % (fileState, path) )

#-=============================================================================-

def EscapeEOL( content, escapeChar = '\\' ):
    def internalEscape( content, escapeChar ):
        return content.replace( r'\n', escapeChar + r'\n' ) \
                      .replace( r'\r', escapeChar + r'\r' )

    if isinstance( content, str ) or isinstance( content, unicode ):
        return internalEscape( content, escapeChar )
    else:
        ret = []
        for c in content:
            ret.append( internalEscape( c, escapeChar) )
        return ret

#-------------------------------------------------------------------------------

def Escape( content, escapeChar = '\\' ):
    def internalEscape( content, escapeChar ):
        return content.replace( '\\', escapeChar + '\\' ) \
                      .replace( '*' , escapeChar + '*'  ) \
                      .replace( '+' , escapeChar + '+'  ) \
                      .replace( '.' , escapeChar + '.'  ) \
                      .replace( '?' , escapeChar + '?'  ) \
                      .replace( '(' , escapeChar + '('  ) \
                      .replace( ')' , escapeChar + ')'  ) \
                      .replace( '{' , escapeChar + '{'  ) \
                      .replace( '}' , escapeChar + '}'  ) \
                      .replace( '[' , escapeChar + '['  ) \
                      .replace( ']' , escapeChar + ']'  )

    if isinstance( content, str ) or isinstance( content, unicode ):
        return internalEscape( content, escapeChar )
    else:
        ret = []
        for c in content:
            ret.append( internalEscape( c, escapeChar) )
        return ret

#-------------------------------------------------------------------------------

def ConvertToSearchRE( plainText ):
    """
        Convert text to a 'search' regular expression

        Adapt the first line to have a capture group for line indent and
        for line break, pre- and postfix respectively. Adapt the remaining
        lines to reference the captured groups.

        usage:
            result = re.search( ConvertToSearchRE( searchForText )
                              , searchInText )
    """
    lines = plainText.splitlines()
    for i in range( len( lines ) ):
        lines[i] = Escape( lines[i] )
        if i == 0:
            lines[0] = r'(?P<INDENT>[ \t]*)' + lines[0] + r'(?P<EOL>\r?\n|\r)'
        else:
            lines[i] = r'(?P=INDENT)'        + lines[i] + r'(?P=EOL)'
    return ''.join( lines )

#-------------------------------------------------------------------------------

def ConvertToReplaceRE( plainText ):
    """
        Convert text to a 'replace' regular expression

        Adapt all lines to reference capture groups for line indent and
        line break.

        usage:
            result = re.sub( searchForText
                           , ConvertToReplaceRE( replaceWithText )
                           , replaceInText )
    """
    lines = plainText.splitlines()
    for i in range( len( lines ) ):
        lines[i] = r'\g<INDENT>' + lines[i] + r'\g<EOL>'
    return ''.join( lines )

#--===========================================================================--

def ReadCategory( data, nameCategory ):

    assert( isinstance( data, str ) or isinstance( data, unicode ) )
    assert( isinstance( nameCategory, str ) )

    preCategory  = None
    dataCategory = None
    postCategory = None

    result = re.search( r'^(?P<PRE>.*?)'
                      + r'(?P<DATA>cat[ \t]+'    + nameCategory + r'.*?)'
                      + r'(?P<POST>cat[ \t]+(?!' + nameCategory + r')?.*)?$'
                      , data, flags = re.DOTALL )
    if result != None:
        preCategory  = result.group( 'PRE'  )
        dataCategory = result.group( 'DATA' )
        postCategory = result.group( 'POST' )

    return dataCategory, preCategory, postCategory

#-------------------------------------------------------------------------------

def AddEntry( dataCategory, entryValue, entryName, entryType, addComment = True ):
    """
        Add entry to category, update when entry exists
    """
    assert( isinstance( dataCategory, str ) or
            isinstance( dataCategory, unicode ) )
    assert( isinstance( entryName, str ) or isinstance( entryName, unicode ) )
    assert( isinstance( entryType, str ) )

    dataOut = dataCategory

    result  = re.search( r'cat[ \t]+(?P<CATEGORY>\w+)[^\r\n]*(?P<EOL>\r?\n|\r)'
                       , dataCategory )
    if result != None:
        nameCategory = result.group( 'CATEGORY' )
        EOL          = result.group( 'EOL' )

        if isinstance( entryValue, str ) or isinstance( entryValue, unicode ):
            entryValueTmp = entryValue
        else:
            entryValueTmp = str( entryValue )

        # <type><ws>+<name><ws>*=<ws>*<data><post><eol>
        # ^-----------pre------------^      ^----^
        # e.g. before
        #           'val KEYWORDSET_MAX=8<EOL>'
        #      and after
        #           'val KEYWORDSET_MAX=30 // [Notepad++]**: Autoupdated<EOL>'
        #
        search = r'(?P<PRE>' + entryType + r'[ \t]+' \
                             + entryName + r'[ \t]*=[ \t]*)' \
               + r'(?P<VALUE>\w+(?:[ \t]*[~!%\^&*+\-|<>/]+[ \t]*\w+)*)' \
               + r'(?P<POST>[^\r\n]*)(?:\r?\n|\r)'
        result = re.search( search, dataCategory )
        if result == None:
            # Entry not found --> add
            dataOut += entryType + ' ' + entryName + '=' + entryValueTmp
            if addComment:
                dataOut += NppComment.AutoGenerated()
            dataOut += EOL

            # Just added a feature, so remove comment when found
            match   = r'#[ \t\w]+no ' + nameCategory +  ' features' \
                    + r'[^\r\n]*(?:\r?\n|\r)+'
            replace = ''
            dataTmp = re.sub( match, replace, dataOut, flags = re.IGNORECASE )
            if dataTmp != dataOut:
                dataOut = dataTmp + EOL

        elif -1 == entryValueTmp.find( result.group( 'VALUE' ) ):
            # Entry found and different value --> update
            #   (i.e. existing entry's value not found in new value)
            preValue  = result.group( 'PRE'  )
            postValue = result.group( 'POST' )

            match    = search
            replace  = preValue + entryValueTmp + postValue
            if addComment and not NppComment.Prefix() in postValue:
                replace += NppComment.AutoUpdated()
            replace += EOL
            dataOut  = re.sub( match, replace, dataOut, flags = re.IGNORECASE )

        #else:
        #   Found and same value --> do nothing

    else:
        print( "ERROR: No category found in data." )

    return dataOut

#-------------------------------------------------------------------------------

def CreateEntry( dataCategory, entryValue, entryName, entryType, addComment = True ):
    """
        Create entry for category
    """
    assert( isinstance( dataCategory, str     ) or
            isinstance( dataCategory, unicode ) )
    assert( isinstance( entryName, str ) or isinstance( entryName, unicode ) )
    assert( isinstance( entryType, str ) )

    dataOut = u'' if isinstance( dataCategory, unicode ) else ''

    result  = re.search( r'cat[ \t]+(?P<CATEGORY>\w+)[^\r\n]*(?P<EOL>\r?\n|\r)'
                       , dataCategory )
    if result != None:
        nameCategory = result.group( 'CATEGORY' )
        EOL          = result.group( 'EOL' )

        if isinstance( entryValue, str ) or isinstance( entryValue, unicode ):
            entryValueTmp = entryValue
        else:
            entryValueTmp = str( entryValue )

        # <type><ws>+<name><ws>*=<ws>*<data><post><eol>
        # ^-----------pre------------^      ^----^
        # e.g. before
        #           'val KEYWORDSET_MAX=8<EOL>'
        #      and after
        #           'val KEYWORDSET_MAX=30 // [Notepad++]**: Autoupdated<EOL>'
        #
        search = r'(?P<PRE>' + entryType + r'[ \t]+'         \
                             + entryName + r'[ \t]*=[ \t]*)' \
               + r'(?P<VALUE>\w+(?:[ \t]*[~!%\^&*+\-|<>/]+[ \t]*\w+)*)' \
               + r'(?P<POST>[^\r\n]*)(?:\r?\n|\r)'
        result = re.search( search, dataCategory )
        if result == None:
            # Entry not found --> add
            dataOut += entryType + ' ' + entryName + '=' + entryValueTmp
            if addComment:
                dataOut += NppComment.AutoGenerated()
            dataOut += EOL

        elif -1 == entryValueTmp.find( result.group( 'VALUE' ) ):
            # Entry found and different value --> update
            #   (i.e. existing entry's value not found in new value)
            preValue  = result.group( 'PRE'  )
            postValue = result.group( 'POST' )

            dataOut  += preValue + entryValueTmp + postValue
            if addComment and not NppComment.Prefix() in postValue:
                dataOut += NppComment.AutoUpdated()
            dataOut  += EOL

        #else:
        #   Found and same value --> do nothing

    else:
        print( "ERROR: No category found in data." )

    return dataOut

#-------------------------------------------------------------------------------

def ReadEntry( dataCategory, entryName, entryType ):
    """
        Read entry from category
    """
    assert( isinstance( dataCategory, str     ) or
            isinstance( dataCategory, unicode ) )
    assert( isinstance( entryName, str ) or isinstance( entryName, unicode ) )
    assert( isinstance( entryType, str ) )

    entryValue = None

    result = re.search( entryType + r'[ \t]+'
                      + entryName + r'[ \t]*=[ \t]*'
                      + r'(?P<VALUE>\w+(?:[ \t]*\w+)*)'
                      , dataCategory )
    if result != None:
        entryValue = result.group( 'VALUE' )

    return entryValue

#-------------------------------------------------------------------------------

##  'enu' feature has the syntax
##      enu<ws>+<name><ws>*=<ws>*<prefix>[<ws>+<prefix>]*
##  where
##      all the 'val' features in this file starting with a given <prefix> are
##      considered part of the enumeration.

def AddEnu( dataCategory, nameValue, dataValue, addComment = True ):
    return AddEntry( dataCategory, dataValue, nameValue, "enu", addComment )

def CreateEnu( dataCategory, nameValue, dataValue, addComment = True ):
    return CreateEntry( dataCategory, dataValue, nameValue, "enu", addComment )

def ReadEnu( dataCategory, nameValue ):
    return ReadEntry( dataCategory, nameValue, "enu" )

#-------------------------------------------------------------------------------

##  'lex' feature has the syntax
##      lex<ws>+<name><ws>*=<ws>*<value><ws>+<prefix>[<ws>+<prefix>]*
##  where
##      <name>  is a reasonably capitalised (Python, XML) identifier or UI name,
##      <value> is the value used to specify the lexer, and the list of
##              prefixes is similar to 'enu' feature. The name is not allowed
##              to be the same as that used within the lexer so the <value>
##              should be used to tie these entities together.

def AddLex( dataCategory, nameValue, dataValue, addComment = True ):
    return AddEntry( dataCategory, dataValue, nameValue, "lex", addComment )

def CreateLex( dataCategory, nameValue, dataValue, addComment = True ):
    return CreateEntry( dataCategory, dataValue, nameValue, "lex", addComment )

def ReadLex( dataCategory, nameValue ):
    return ReadEntry( dataCategory, nameValue, "lex" )

#-------------------------------------------------------------------------------

##  'val' feature has the syntax
##      val<ws>+<name><ws>*=<ws>*<value>

def AddVal( dataCategory, nameValue, dataValue, addComment = True ):
    return AddEntry( dataCategory, dataValue, nameValue, "val", addComment )

def CreateVal( dataCategory, nameValue, dataValue, addComment = False ):
    return CreateEntry( dataCategory, dataValue, nameValue, "val", addComment )

def ReadVal( dataCategory, nameValue ):
    return ReadEntry( dataCategory, nameValue, "val" )

#--===========================================================================--

def AddLexer( dataIn, lexerName ):

    assert( isinstance( dataIn, str ) or isinstance( dataIn, unicode ) )
    assert( isinstance( lexerName, str ) or isinstance( lexerName, unicode ) )

    dataOut = dataIn

    # e.g.  <Start-of-Category>             <-- Head start
    #        :
    #       enu Lexer=SCLEX_<EOL>
    #       val SCLEX_CONTAINER=0<EOL>
    #       val SCLEX_NULL=1<EOL>
    #        :                              <-- Head end
    #       val SCLEX_TEHEX=119<EOL>        <-- Last
    #        :                              <-- Foot start
    #       <End-of-Category>               <-- Foot end
    #
    result = re.search( r'^(?P<HEAD>.*enu[ \t]+Lexer=(?P<PREFIX>[A-Z]{2,}_)'
                      +         r'[^\r\n]*(?P<EOL>\r?\n|\r)'
                      + r'(?:val[ \t]+(?P=PREFIX)[A-Z0-9]+[ \t]*=[ \t]*\d+'
                      +         r'[^\r\n]*(?P=EOL))*)'
                      + r'(?P<LAST>val[ \t]+(?P=PREFIX)[A-Z0-9]+[ \t]*='
                      +         r'[ \t]*\w+[^\r\n]*(?P=EOL))'
                      + r'(?P<FOOT>.*)$'
                      , dataIn, flags = re.DOTALL )
    if result != None:
        EOL       = result.group( 'EOL' )
        nameValue = result.group( 'PREFIX' ) + lexerName.upper()

        if nameValue not in dataIn:
            lastValue     = result.group( 'LAST' )
            # split( '/' )[0] to discard trailing comment
            lastValueData = lastValue.split( '=' )[1].split( '/' )[0].rstrip()
            dataValue     = str( int( lastValueData ) + 1 )

            dataOut  = result.group( 'HEAD' )
            dataOut += lastValue
            dataOut += CreateVal( dataOut, nameValue, dataValue
                                , addComment = True )
            dataOut += result.group( 'FOOT' )

            # e.g.  <Start-of-Category>
            #        :
            #       enu Lexer=SCLEX_<EOL>
            #       val SCLEX_CONTAINER=0<EOL>
            #       val SCLEX_NULL=1<EOL>
            #        :
            #       val SCLEX_TEHEX=119<EOL>
            #       val SCLEX_SEARCHRESULT=120<EOL> // [Notepad++]~~: ...
            #        :
            #       <End-of-Category>

        #else:
        #   lexer value already defined

    return dataOut

#-------------------------------------------------------------------------------

def CreateLexicalHeader( dataIn, lexerName ):

    assert( isinstance( dataIn, str ) or isinstance( dataIn, unicode ) )
    assert( isinstance( lexerName, str ) or isinstance( lexerName, unicode ) )

    dataOut = u'' if isinstance( dataIn, unicode ) else ''

    lexerValue = ReadLex( dataIn, lexerName )
    if lexerValue == None:
        #---
        lexerPrefix = ReadEnu( dataIn, "Lexer" )
        if lexerPrefix == None:
            lexerPrefix = 'SCLEX_'
        #---
        # e.g. '# Lexical state for SCLEX_SEARCHRESULT'
        #      'lex SearchResult=SCLEX_SEARCHRESULT SCE_SEARCHRESULT_'
        dataOut += '# Lexical state for %(PREFIX)s%(LEXER)s%(EOL)s' % \
                         { "PREFIX": lexerPrefix
                         , "LEXER" : lexerName.upper()
                         , "EOL"   : lineEnd }
        dataOut += CreateLex( dataIn
                            , lexerName
                            , '%(PREFIX)s%(LEXER)s SCE_%(LEXER)s_' % \
                                    { "PREFIX" : lexerPrefix
                                    , "LEXER"  : lexerName.upper() }
                            , addComment = False )

    return dataOut

#-------------------------------------------------------------------------------

def CreateLexicalStates( dataIn, lexerName, states ):

    assert( isinstance( dataIn, str ) or isinstance( dataIn, unicode ) )
    assert( isinstance( lexerName, str ) or isinstance( lexerName, unicode ) )

    dataOut = u'' if isinstance( dataIn, unicode ) else ''

    idx = 0
    for state in states:
        if isinstance( state, str ) or isinstance( state, unicode ):
            dataOut += CreateVal( dataIn
                                , 'SCE_%(LEXER)s_%(STATE)s' %
                                        { "LEXER" : lexerName.upper()
                                        , "STATE" : state.upper() }
                                , idx )
            idx += 1

        elif isinstance( state, tuple ):
            stateName = state[0]
            stateData = state[1]
            if not isinstance( stateData, str     ) and \
               not isinstance( stateData, unicode ):
                stateData = str( stateData )
            dataOut += CreateVal( dataIn
                                , 'SCE_%(LEXER)s_%(STATE)s' %
                                        { "LEXER" : lexerName.upper()
                                        , "STATE" : stateName.upper() }
                                , stateData )

    return dataOut

#-------------------------------------------------------------------------------

def CreateEnumeratorValues( dataIn, enumName, values ):

    assert( isinstance( dataIn, str ) or isinstance( dataIn, unicode ) )
    assert( isinstance( enumName, str ) or isinstance( enumName, unicode ) )

    dataOut = u'' if isinstance( dataIn, unicode ) else ''

    #---
    enumPrefix = ReadEnu( dataIn, enumName )
    if enumPrefix == None:
        enumPrefix = 'SCE_' + enumName.upper() + '_'

    #---
    idx = 0
    for value in values:
        if isinstance( value, str ) or isinstance( value, unicode ):
            dataOut += CreateVal( dataIn
                                , '%(PREFIX)s%(VALUE)s' %
                                        { "PREFIX" : enumPrefix.upper()
                                        , "VALUE"  : value.upper() }
                                , idx )
            idx += 1

        elif isinstance( value, tuple ):
            valueName = value[0]
            valueData = value[1]
            if not isinstance( valueData, str     ) and \
               not isinstance( valueData, unicode ):
                valueData = str( valueData )
            dataOut += CreateVal( dataIn
                                , '%(PREFIX)s%(VALUE)s' %
                                        { "PREFIX" : enumPrefix.upper()
                                        , "VALUE"  : valueName.upper() }
                                , valueData )

    return dataOut

#--===========================================================================--

def InsertCode( codeIn, insertCode, insertAfter = None, insertBefore = None ):

    assert( isinstance( codeIn, str ) or isinstance( codeIn, unicode ) )
    assert( isinstance( insertCode, str ) or isinstance( insertCode, unicode ) )

    codeOut  = codeIn
    sarFlags = re.DOTALL # search and replace flags

    # Was the code already added?
    search = ConvertToSearchRE( insertCode )
    result = re.search( search, codeIn, flags = sarFlags )
    if result == None:
        # Not found --> Add

        if insertAfter == None and insertBefore == None:
            # Append
            codeOut += NppComment.AutoGeneratedBegin()  \
                     + insertCode                       \
                     + NppComment.AutoGeneratedEnd()

        else:
            # Insert
            match    = r'(?P<INDENT>[ \t]*)'
            replace  = r'\g<INDENT>'

            if insertAfter != None:
                match   += r'(?P<INSERTAFTER>'  + insertAfter  + ')'
                replace += r'\g<INSERTAFTER>\g<EOL>' \
                         + r'\g<EOL>'

            replace += NppComment.AutoGeneratedBegin()  \
                     + ConvertToReplaceRE( insertCode ) \
                     + NppComment.AutoGeneratedEnd()

            if insertBefore != None:
                match   += r'(?P<INSERTBEFORE>' + insertBefore + ')'
                replace += r'\g<INSERTBEFORE>\g<EOL>'

            match   += r'(?P<EOL>\r?\n|\r)'

            codeOut  = re.sub( match, replace, codeIn, flags = sarFlags )

    #else:
    #   code already present

    return codeOut

#-------------------------------------------------------------------------------

def UpdateLexHTMLDotCxx( rootScintilla ):
    """
        Update LexHTML.cxx
    """
    #---
    path     = rootScintilla + "lexers\LexHTML.cxx"
    dataRead = ReadFile( path )
    if dataRead != None:
        dataUpdated = dataRead
        #---
        # Disable 'External Script' scripting indicator
        result = re.search( r'(?P<FUNCTION>'
                          + r'^static script_type segIsScriptingIndicator'
                          + r'[^{]+\{'          # start of body
                          + r'.+?'              # body
                          + r'^\}'              # end of body
                          + r')'
                          , dataUpdated
                          , flags = re.DOTALL | re.MULTILINE )
        if result != None:
            functionRead    = result.group( 'FUNCTION' )
            functionUpdated = functionRead
            #---
            match   = r'(?P<LINE1>'                                     \
                    +   r'(?P<INDENT>[ \t]+)if \(strstr\(s, "src"\)\)'  \
                                + r'[^\r\n]+(?P<EOL>\r?\n|\r)'          \
                    + r')'                                              \
                    + r'(?P<LINE2>'                                     \
                    +   r'(?P=INDENT)\treturn eScriptNone;(?P=EOL)'     \
                    + r')'
            replace = NppComment.AutoUpdatedBegin() \
                    + r'//\g<LINE1>'                \
                    + r'//\g<LINE2>'                \
                    + NppComment.AutoUpdatedEnd()
            functionUpdated = re.sub( match, replace, functionUpdated )
            #---
            match   = Escape( functionRead )
            replace = EscapeEOL( functionUpdated )
            dataUpdated = re.sub( match, replace, dataUpdated )
        #---
        # Swap first state to reach when entering a scripting language
        result = re.search( r'(?P<FUNCTION>'
                          + r'(?P<INDENT>[ \t]*)static int StateForScript'
                          + r'[^{]+\{'          # start of body
                          + r'[^{]+\{[^}]+\}'   # switch statement
                          + r'(?P=INDENT)'
                          + r'[^}]*\}'          # end of body
                          + r')'
                          , dataUpdated, flags = re.DOTALL )
        if result != None:
            functionRead    = result.group( 'FUNCTION' )
            functionUpdated = functionRead
            #---
            match   = r'(?P<INDENT>[ \t]*)case eScriptVBS:(?P<EOL>\r?\n|\r)' \
                    + r'(?P=INDENT)\tResult = SCE_HB_START;(?P=EOL)'
            replace = NppComment.AutoUpdatedBegin()                 \
                    + r'\g<INDENT>case eScriptJS:\g<EOL>'           \
                    + r'\g<INDENT>\tResult = SCE_HJ_START;\g<EOL>'  \
                    + NppComment.AutoUpdatedEnd()
            functionUpdated = re.sub( match, replace, functionUpdated )
            #---
            match   = r'(?P<INDENT>[ \t]*)default :(?P<EOL>\r?\n|\r)' \
                    + r'(?P=INDENT)\tResult = SCE_HJ_START;(?P=EOL)'
            replace = r'\g<INDENT>default :\g<EOL>'         \
                    + r'\g<INDENT>\tResult = SCE_HB_START;' \
                    + NppComment.AutoUpdated()              \
                    + r'\g<EOL>'
            functionUpdated = re.sub( match, replace, functionUpdated )
            #---
            match   = Escape( functionRead )
            replace = functionUpdated
            dataUpdated = re.sub( match, replace, dataUpdated )
        #---
        # Set default embedded script language
        match   = r'(?P<INDENT>[ \t]*)'                                      \
                + r'(?P<HEAD>[^\r\n]*"asp\.default\.language"[ \t]*,[ \t]*)' \
                + r'eScriptJS'                                               \
                + r'(?P<FOOT>[^\r\n]*)(?P<EOL>\r?\n|\r)'
        replace = r'\g<INDENT>' \
                + r'\g<HEAD>'   \
                + r'eScriptVBS' \
                + r'\g<FOOT>' + NppComment.AutoUpdated() + '\g<EOL>'
        dataUpdated = re.sub( match, replace, dataUpdated )
        #---
        UpdateFile( path, dataUpdated )

#--===========================================================================--

def CorrectScintillaDotIFace( rootScintilla ):
    """
        Update Scintilla.iface
    """
    #---
    # when applicable swap line position of 'empty category'-message and
    #   start of category
    path     = rootScintilla + "include\Scintilla.iface"
    dataRead = ReadFile( path )
    if dataRead != None:
        match   = r'(?P<MESSAGE>'                                   \
                + r'#[ \t\w]+no [Pp]rovisional features[ \t\w]+'    \
                + r')(?P<EOL>(?:\r?\n|\r))+'                        \
                + r'(?P<CATEGORY>cat Provisional)(?P=EOL)'
        replace = r'\g<CATEGORY>\g<EOL>\g<EOL>\g<MESSAGE>\g<EOL>'
        dataUpdated = re.sub( match, replace, dataRead, flags = re.DOTALL )
        #---
        UpdateFile( path, dataUpdated )

#-------------------------------------------------------------------------------

def AddNotifications( rootScintilla ):
    """
        Update Scintilla.iface
    """
    #---
    path     = rootScintilla + "include\Scintilla.iface"
    dataRead = ReadFile( path )
    if dataRead != None:
        dataUpdated = dataRead
        #---
        dataCategory, preCategory, postCategory \
                    = ReadCategory( dataUpdated, "Provisional" )
        if dataCategory != None:
            dataUpdated = dataCategory
            #---
            dataTmp  = CreateVal( dataUpdated
                                , "SCN_SCROLLED"           , 2080 )
            dataTmp += CreateVal( dataUpdated
                                , "SCN_FOLDINGSTATECHANGED", 2081 )
            if dataTmp != u'' and dataTmp != '' and \
               dataTmp not in dataUpdated:
                dataUpdated += NppComment.AutoGeneratedBegin( '#' )
                dataUpdated += dataTmp
                dataUpdated += NppComment.AutoGeneratedEnd( '#' )
                dataUpdated += lineEnd
            #---
            dataUpdated = preCategory + dataUpdated + postCategory
        #---
        UpdateFile( path, dataUpdated )

#-------------------------------------------------------------------------------

def AddMainWindowFocusOnMiddleButtonDown( rootScintilla ):
    """
        Update ScintillaWin.cxx
    """
    #---
    path     = rootScintilla + "win32\ScintillaWin.cxx"
    dataRead = ReadFile( path )
    if dataRead != None:
        insertAfter = r'case[ \t]+WM_LBUTTONDOWN[ \t]*:[ \t]*.*?break;'
        insertCode  = r'case WM_MBUTTONDOWN:'                       + lineEnd \
                    + r'\t::SetFocus(MainHWND());'                  + lineEnd \
                    + r'\tbreak;'                                   + lineEnd
        dataUpdated = InsertCode( dataRead, insertCode, insertAfter )
        #---
        UpdateFile( path, dataUpdated )

#-------------------------------------------------------------------------------

def AddSearchResultLexer( rootScintilla ):
    """
        Search Result lexer
    """
    lexerName = "SearchResult"
    #---
    print( "Adding support for lexer `%s`" % lexerName )
    #---
    path     = rootScintilla + "include\Scintilla.iface"
    dataRead = ReadFile( path )
    if dataRead != None:
        dataUpdated = dataRead
        #---
        dataCategory, preCategory, postCategory \
                    = ReadCategory( dataUpdated, "Basics" )
        if dataCategory != None:
            dataUpdated = AddVal( dataCategory
                                , "SC_SEARCHRESULT_LINEBUFFERMAXLENGTH"
                                , 1024 )
            #---
            dataUpdated = AddLexer( dataUpdated, lexerName )
            #---
            dataTmp   = CreateLexicalHeader( dataUpdated, lexerName )
            #---
            lexerData = { "dataIn"    : dataUpdated
                        , "lexerName" : lexerName
                        , "states"    : [ 'DEFAULT'
                                        , 'SEARCH_HEADER'
                                        , 'FILE_HEADER'
                                        , 'LINE_NUMBER'
                                        , 'WORD2SEARCH'
                                        , 'HIGHLIGHT_LINE'
                                        , 'CURRENT_LINE'
                                        ]
                        }
            dataTmp  += CreateLexicalStates( **lexerData )
            if dataTmp != u'' and dataTmp != '' and \
               dataTmp not in dataUpdated:
                dataUpdated += NppComment.AutoGeneratedBegin( '#' )
                dataUpdated += dataTmp
                dataUpdated += NppComment.AutoGeneratedEnd( '#' )
                dataUpdated += lineEnd
            #---
            dataUpdated = preCategory + dataUpdated + postCategory
        #---
        UpdateFile( path, dataUpdated )

    #---
    path     = rootScintilla + "include\Scintilla.h"
    dataRead = ReadFile( path )
    if dataRead != None:
        #---
        insertAfter = r'struct[ \t]+SCNotification[ \t]+\{.*?\};'
        insertCode  = r'struct SearchResultMarking {'        + lineEnd \
                    + r'\tlong _start;'                      + lineEnd \
                    + r'\tlong _end;'                        + lineEnd \
                    + r'};'                                  + lineEnd \
                    +                                          lineEnd \
                    + r'struct SearchResultMarkings {'       + lineEnd \
                    + r'\tlong _length;'                     + lineEnd \
                    + r'\tSearchResultMarking *_markings;'   + lineEnd \
                    + r'};'                                  + lineEnd
        dataUpdated = InsertCode( dataRead, insertCode, insertAfter )
        #---
        UpdateFile( path, dataUpdated )

    #---
    CopyFiles( ".\Convert2Sci4Npp\lexers\LexSearchResult.cxx"
             , rootScintilla + "lexers" )

#-------------------------------------------------------------------------------

def AddObjectiveCLexer( rootScintilla ):
    """
        Objective-C lexer
    """
    lexerName = "ObjC"
    #---
    print( "Adding support for lexer `%s`" % lexerName )
    #---
    path     = rootScintilla + "include\Scintilla.iface"
    dataRead = ReadFile( path )
    if dataRead != None:
        dataUpdated = dataRead
        #---
        dataCategory, preCategory, postCategory \
                    = ReadCategory( dataUpdated, "Basics" )
        if dataCategory != None:
            dataUpdated = dataCategory
            #---
            dataUpdated = AddLexer( dataUpdated, lexerName )
            #---
            dataTmp   = CreateLexicalHeader( dataUpdated, lexerName )
            #---
            lexerData = { "dataIn"    : dataUpdated
                        , "lexerName" : lexerName
                        , "states"    : [ ( 'DIRECTIVE', 20 )
                                        , ( 'QUALIFIER', 21 )
                                        ]
                        }
            dataTmp  += CreateLexicalStates( **lexerData )
            if dataTmp != u'' and dataTmp != '' and \
               dataTmp not in dataUpdated:
                dataUpdated += NppComment.AutoGeneratedBegin( '#' )
                dataUpdated += dataTmp
                dataUpdated += NppComment.AutoGeneratedEnd( '#' )
                dataUpdated += lineEnd
            #---
            dataUpdated = preCategory + dataUpdated + postCategory
        #---
        UpdateFile( path, dataUpdated )

    #---
    CopyFiles( ".\Convert2Sci4Npp\lexers\LexObjC.cxx"
             , rootScintilla + "lexers" )

#-------------------------------------------------------------------------------

def AddUserDefinedLanguageLexer( rootScintilla ):
    """
        User Defined Language lexer
    """
    lexerName = "UDL"
    #---
    print( "Adding support for lexer `%s`" % lexerName )
    #---
    path     = rootScintilla + "include\Scintilla.iface"
    dataRead = ReadFile( path )
    if dataRead != None:
        dataUpdated = dataRead
        #---
        dataCategory, preCategory, postCategory \
                    = ReadCategory( dataUpdated, "Basics" )
        if dataCategory != None:
            dataUpdated = AddVal( dataCategory, "KEYWORDSET_MAX", 30 )
            #---
            dataUpdated = AddLexer( dataUpdated, lexerName )
            #---
            dataTmp   = CreateLexicalHeader( dataUpdated, lexerName )
            #---
            lexerData = { "dataIn"    : dataUpdated
                        , "lexerName" : lexerName
                        , "states"    :
                            [ ( 'TOTAL_DELIMITERS'    , 8 )
                            , ( 'TOTAL_KEYWORD_GROUPS', 8 )
                            , ( 'TOTAL_FOLDERS'       , 6 )
                            ]
                        }
            dataTmp  += CreateLexicalStates( **lexerData )
            #---
            lexerData = { "dataIn"    : dataUpdated
                        , "lexerName" : lexerName
                        , "states"    :
                            [ 'KWLIST_COMMENTS'
                            , 'KWLIST_NUMBER_PREFIX1'
                            , 'KWLIST_NUMBER_PREFIX2'
                            , 'KWLIST_NUMBER_EXTRAS1'
                            , 'KWLIST_NUMBER_EXTRAS2'
                            , 'KWLIST_NUMBER_SUFFIX1'
                            , 'KWLIST_NUMBER_SUFFIX2'
                            , 'KWLIST_NUMBER_RANGE'
                            , 'KWLIST_OPERATORS1'
                            , 'KWLIST_OPERATORS2'
                            , 'KWLIST_FOLDERS_IN_CODE1_OPEN'
                            , 'KWLIST_FOLDERS_IN_CODE1_MIDDLE'
                            , 'KWLIST_FOLDERS_IN_CODE1_CLOSE'
                            , 'KWLIST_FOLDERS_IN_CODE2_OPEN'
                            , 'KWLIST_FOLDERS_IN_CODE2_MIDDLE'
                            , 'KWLIST_FOLDERS_IN_CODE2_CLOSE'
                            , 'KWLIST_FOLDERS_IN_COMMENT_OPEN'
                            , 'KWLIST_FOLDERS_IN_COMMENT_MIDDLE'
                            , 'KWLIST_FOLDERS_IN_COMMENT_CLOSE'
                            , 'KWLIST_KEYWORDS1'
                            , 'KWLIST_KEYWORDS2'
                            , 'KWLIST_KEYWORDS3'
                            , 'KWLIST_KEYWORDS4'
                            , 'KWLIST_KEYWORDS5'
                            , 'KWLIST_KEYWORDS6'
                            , 'KWLIST_KEYWORDS7'
                            , 'KWLIST_KEYWORDS8'
                            , 'KWLIST_DELIMITERS'
                            ]
                        }
            dataTmp  += CreateLexicalStates( **lexerData )
            #---
            lexerData = { "dataIn"    : dataUpdated
                        , "lexerName" : lexerName
                        , "states"    :
                            [ ( 'KWLIST_TOTAL'
                              , 'SCE_' + lexerName.upper()
                                + '_KWLIST_DELIMITERS+1 // must'
                                + ' always be <= KEYWORDSET_MAX'
                              )
                            ]
                        }
            dataTmp  += CreateLexicalStates( **lexerData )
            #---
            lexerData = { "dataIn"    : dataUpdated
                        , "lexerName" : lexerName
                        , "states"    :
                            [ 'STYLE_DEFAULT'
                            , 'STYLE_COMMENT'
                            , 'STYLE_COMMENTLINE'
                            , 'STYLE_NUMBER'
                            , 'STYLE_KEYWORD1'
                            , 'STYLE_KEYWORD2'
                            , 'STYLE_KEYWORD3'
                            , 'STYLE_KEYWORD4'
                            , 'STYLE_KEYWORD5'
                            , 'STYLE_KEYWORD6'
                            , 'STYLE_KEYWORD7'
                            , 'STYLE_KEYWORD8'
                            , 'STYLE_OPERATOR'
                            , 'STYLE_FOLDER_IN_CODE1'
                            , 'STYLE_FOLDER_IN_CODE2'
                            , 'STYLE_FOLDER_IN_COMMENT'
                            , 'STYLE_DELIMITER1'
                            , 'STYLE_DELIMITER2'
                            , 'STYLE_DELIMITER3'
                            , 'STYLE_DELIMITER4'
                            , 'STYLE_DELIMITER5'
                            , 'STYLE_DELIMITER6'
                            , 'STYLE_DELIMITER7'
                            , 'STYLE_DELIMITER8'
                            , 'STYLE_IDENTIFIER'
                            ]
                        }
            dataTmp  += CreateLexicalStates( **lexerData )
            #---
            lexerData = { "dataIn"    : dataUpdated
                        , "lexerName" : lexerName
                        , "states"    :
                            [ ( 'STYLE_TOTAL_STYLES', 'SCE_'
                                + lexerName.upper()
                                + '_STYLE_IDENTIFIER // must always'
                                + ' equal the last SCE_UDL_STYLE_'
                              )
                            , ( 'STYLE_MAPPER_TOTAL', '17' )
                            , ( 'STYLE_ARRAY_SIZE'  , '30 // must'
                                + ' cover sizes of NppParameters::'
                                + '_lexerStyler and NppParameters::'
                                + '_widgetStyle'
                              )
                            ]
                        }
            dataTmp  += CreateLexicalStates( **lexerData )
            #---
            lexerData = { "dataIn"    : dataUpdated
                        , "lexerName" : lexerName
                        , "states"    :
                            [ ( 'MASK_NESTING_NONE'        , '0x00000000' )
                            , ( 'MASK_NESTING_DELIMITER1'  , '0x00000001' )
                            , ( 'MASK_NESTING_DELIMITER2'  , '0x00000002' )
                            , ( 'MASK_NESTING_DELIMITER3'  , '0x00000004' )
                            , ( 'MASK_NESTING_DELIMITER4'  , '0x00000008' )
                            , ( 'MASK_NESTING_DELIMITER5'  , '0x00000010' )
                            , ( 'MASK_NESTING_DELIMITER6'  , '0x00000020' )
                            , ( 'MASK_NESTING_DELIMITER7'  , '0x00000040' )
                            , ( 'MASK_NESTING_DELIMITER8'  , '0x00000080' )
                            , ( 'MASK_NESTING_COMMENT'     , '0x00000100' )
                            , ( 'MASK_NESTING_COMMENT_LINE', '0x00000200' )
                            , ( 'MASK_NESTING_KEYWORD1'    , '0x00000400' )
                            , ( 'MASK_NESTING_KEYWORD2'    , '0x00000800' )
                            , ( 'MASK_NESTING_KEYWORD3'    , '0x00001000' )
                            , ( 'MASK_NESTING_KEYWORD4'    , '0x00002000' )
                            , ( 'MASK_NESTING_KEYWORD5'    , '0x00004000' )
                            , ( 'MASK_NESTING_KEYWORD6'    , '0x00008000' )
                            , ( 'MASK_NESTING_KEYWORD7'    , '0x00010000' )
                            , ( 'MASK_NESTING_KEYWORD8'    , '0x00020000' )
                            , ( 'MASK_NESTING_FOLDERS_IN_CODE2_OPEN'
                                                           , '0x00040000' )
                            , ( 'MASK_NESTING_FOLDERS_IN_CODE2_MIDDLE'
                                                           , '0x00080000' )
                            , ( 'MASK_NESTING_FOLDERS_IN_CODE2_CLOSE'
                                                           , '0x00100000' )
                            , ( 'MASK_NESTING_FOLDERS_IN_COMMENT_OPEN'
                                                           , '0x00200000' )
                            , ( 'MASK_NESTING_FOLDERS_IN_COMMENT_MIDDLE'
                                                           , '0x00400000' )
                            , ( 'MASK_NESTING_FOLDERS_IN_COMMENT_CLOSE'
                                                           , '0x00800000' )
                            , ( 'MASK_NESTING_OPERATORS1'  , '0x01000000' )
                            , ( 'MASK_NESTING_OPERATORS2'  , '0x02000000' )
                            , ( 'MASK_NESTING_NUMBERS'     , '0x04000000' )
                            ]
                        }
            dataTmp  += CreateLexicalStates( **lexerData )
            #---
            lexerData = { "dataIn"    : dataUpdated
                        , "lexerName" : lexerName
                        , "states"    : [ ( 'VERSION_MAJOR'   ,  2 )
                                        , ( 'VERSION_MINOR'   ,  1 )
                                        , ( 'VERSION_BUILD'   ,  0 )
                                        , ( 'VERSION_REVISION', 12 )
                                        ]
                        }
            dataTmp  += CreateLexicalStates( **lexerData )
            #---
            if dataTmp != u'' and dataTmp != '' and \
               dataTmp not in dataUpdated:
                dataUpdated += NppComment.AutoGeneratedBegin( '#' )
                dataUpdated += dataTmp
                dataUpdated += NppComment.AutoGeneratedEnd( '#' )
                dataUpdated += lineEnd
            #---
            dataUpdated = preCategory + dataUpdated + postCategory
        #---
        UpdateFile( path, dataUpdated )

    #---
    CopyFiles( ".\Convert2Sci4Npp\lexers\LexUser.cxx"
             , rootScintilla + "lexers" )

    #---
    path     = rootScintilla + "lexers\LexUser.cxx"
    dataRead = ReadFile( path )
    if dataRead != None:
        dataUpdated = dataRead
        #---
        match   = 'SCLEX_USER'
        replace = 'SCLEX_%s' % lexerName.upper()
        dataUpdated = re.sub( match, replace, dataUpdated )
        #---
        match   = 'SCE_USER_'
        replace = 'SCE_%s_' % lexerName.upper()
        dataUpdated = re.sub( match, replace, dataUpdated )
        #---
        UpdateFile( path, dataUpdated )

    #---
    path     = rootScintilla + "lexlib\StyleContext.h"
    dataRead = ReadFile( path )
    if dataRead != None:
        insertAfter  = r'bool[ \t]+MatchIgnoreCase[ \t]*\([ \t]*const[ \t]+'  \
                            + r'char[ \t]*\*[ \t]*s[ \t]*\)[ \t]*\{.*?\}'
        insertBefore = r'\s+// Non-inline'
        insertCode   = r'bool MatchIgnoreCase2(const char *s) {'    + lineEnd \
                     + r'\tif (MakeLowerCase(ch) != MakeLowerCase('           \
                            + r'static_cast<unsigned char>(*s)))'   + lineEnd \
                     + r'\t\treturn false;'                         + lineEnd \
                     + r'\ts++;'                                    + lineEnd \
                     + r'\tif (!*s)'                                + lineEnd \
                     + r'\t\treturn true;'                          + lineEnd \
                     + r'\tif (MakeLowerCase(chNext) != MakeLowerCase('       \
                            + r'static_cast<unsigned char>(*s)))'   + lineEnd \
                     + r'\t\treturn false;'                         + lineEnd \
                     + r'\ts++;'                                    + lineEnd \
                     + r'\tfor (int n = 2; *s; n++) {'              + lineEnd \
                     + r'\t\tif (MakeLowerCase(static_cast<unsigned '         \
                            + r'char>(*s)) !='                      + lineEnd \
                     + r'\t\t\tMakeLowerCase(static_cast<unsigned '           \
                            + r'char>(styler.SafeGetCharAt(currentPos'        \
                            + r' + n))))'                           + lineEnd \
                     + r'\t\t\treturn false;'                       + lineEnd \
                     + r'\t\ts++;'                                  + lineEnd \
                     + r'\t}'                                       + lineEnd \
                     + r'\treturn true;'                            + lineEnd \
                     + r'}'                                         + lineEnd
        dataUpdated = InsertCode( dataRead
                                , insertCode, insertAfter, insertBefore )
        #---
        UpdateFile( path, dataUpdated )

    #---
    path     = rootScintilla + "lexlib\WordList.h"
    dataRead = ReadFile( path )
    if dataRead != None:
        insertAfter = r'const[ \t]+char[ \t]*\*[ \t]*WordAt[ \t]*\([ \t]*'    \
                            + r'int[ \t]+n[ \t]*\)([ \t]*const;)?'
        insertCode  = r'void SetWordAt(int n, const char *'                   \
                            + r'word2Set) {'                        + lineEnd \
                    + r'\twords[n] = (char *)word2Set;'             + lineEnd \
                    + r'};'                                         + lineEnd \
                    + r'int StartAt(int n) const {'                 + lineEnd \
                    + r'\treturn starts[n];'                        + lineEnd \
                    + r'};'                                         + lineEnd
        dataUpdated = InsertCode( dataRead, insertCode, insertAfter )
        #---
        UpdateFile( path, dataUpdated )

#-------------------------------------------------------------------------------

def AddUniversalLexerStates( rootScintilla ):
    """
        for ALL lexers
    """
    lexerName = "UNIVERSAL"
    #---
    path     = rootScintilla + "include\Scintilla.iface"
    dataRead = ReadFile( path )
    if dataRead != None:
        dataUpdated = dataRead
        #---
        dataCategory, preCategory, postCategory \
                    = ReadCategory( dataUpdated, "Basics" )
        if dataCategory != None:
            dataUpdated = dataCategory
            #---
            lexerData = { "dataIn"    : dataUpdated
                        , "lexerName" : lexerName
                        , "states"    : [ ( 'FOUND_STYLE'      , 31 )
                                        , ( 'FOUND_STYLE_SMART', 29 )
                                        , ( 'FOUND_STYLE_INC'  , 28 )
                                        , ( 'TAGMATCH'         , 27 )
                                        , ( 'TAGATTR'          , 26 )
                                        , ( 'FOUND_STYLE_EXT1' , 25 )
                                        , ( 'FOUND_STYLE_EXT2' , 24 )
                                        , ( 'FOUND_STYLE_EXT3' , 23 )
                                        , ( 'FOUND_STYLE_EXT4' , 22 )
                                        , ( 'FOUND_STYLE_EXT5' , 21 )
                                        ]
                        }
            dataTmp   = CreateLexicalStates( **lexerData )
            if dataTmp != u'' and dataTmp != '' and \
               dataTmp not in dataUpdated:
                dataUpdated += NppComment.AutoGeneratedBegin( '#' )
                dataUpdated += dataTmp
                dataUpdated += NppComment.AutoGeneratedEnd( '#' )
                dataUpdated += lineEnd
            #---
            dataUpdated = preCategory + dataUpdated + postCategory
        #---
        UpdateFile( path, dataUpdated )

#-------------------------------------------------------------------------------

def AddBoostRegExSearch( rootScintilla, rootBoost, toolset ):
    """
        Add support for Boost::RegEx Search
    """
    #---
    path     = rootScintilla + "include\Scintilla.iface"
    dataRead = ReadFile( path )
    if dataRead != None:
        dataUpdated = dataRead
        #---
        dataCategory, preCategory, postCategory \
                    = ReadCategory( dataUpdated, "Basics" )
        if dataCategory != None:
            dataUpdated = dataCategory
            #---
            enumData = { "dataIn"   : dataUpdated
                       , "enumName" : "FindOption"
                       , "values"   :
                            [ ( 'REGEXP_DOTMATCHESNL'       , '0x10000000' )
                            , ( 'REGEXP_EMPTYMATCH_MASK'    , '0xE0000000' )
                            , ( 'REGEXP_EMPTYMATCH_NONE'    , '0x00000000' )
                            , ( 'REGEXP_EMPTYMATCH_NOTAFTERMATCH'
                                                            , '0x20000000' )
                            , ( 'REGEXP_EMPTYMATCH_ALL'     , '0x40000000' )
                            , ( 'REGEXP_EMPTYMATCH_ALLOWATSTART'
                                                            , '0x80000000' )
                            , ( 'REGEXP_SKIPCRLFASONE'      , '0x08000000' )
                            ]
                       }
            dataTmp   = CreateEnumeratorValues( **enumData )
            if dataTmp != u'' and dataTmp != '' and \
               dataTmp not in dataUpdated:
                dataUpdated += NppComment.AutoGeneratedBegin( '#' )
                dataUpdated += dataTmp
                dataUpdated += NppComment.AutoGeneratedEnd( '#' )
                dataUpdated += lineEnd
            #---
            dataUpdated = preCategory + dataUpdated + postCategory
        #---
        UpdateFile( path, dataUpdated )

    #---
    CopyFiles( ".\Convert2Sci4Npp\src\*.*"
             , rootScintilla + "src" )

    #---
    path     = rootScintilla + "win32\scintilla.mak"
    dataRead = ReadFile( path )
    if dataRead != None:
        dataUpdated = dataRead
        #---
        match   = r'(?P<SHAREDOBJSHEAD>'                    \
                +   r'SHAREDOBJS=[ \t]*\\(?P<EOL>\r?\n|\r)' \
                + r')'                                      \
                + r'(?P<SHAREDOBJSBODY>'                    \
                +   r'(?:'                                  \
                +     r'(?P<INDENT>[ \t]+)'                 \
                +     r'(?P<OBJDIR>\$\(\w+\)[\\/])'         \
                +     r'\w+\.(?:OBJ|obj|Obj)'               \
                +     r'(?P<CONT>[ \t]*\\)(?P=EOL)'         \
                +   r')*'                                   \
                + r')'
        replace = r'\g<SHAREDOBJSHEAD>'                           \
                + NppComment.AutoGeneratedBegin( '#' )            \
                + r'\g<INDENT>\g<OBJDIR>BoostRegexSearch.obj'     \
                                            + r'\g<CONT>\g<EOL>'  \
                + r'\g<INDENT>\g<OBJDIR>UTF8DocumentIterator.obj' \
                                            + r'\g<CONT>\g<EOL>'  \
                + NppComment.AutoGeneratedEnd( '#' )              \
                + r'\g<SHAREDOBJSBODY>'
        dataUpdated = re.sub( match, replace, dataUpdated )
        #---
        match   = r'(?P<INCLUDEDIRS>INCLUDEDIRS=[^\r\n]+)'
        replace = r'\g<INCLUDEDIRS>'                      \
                + r' -I' + rootBoost.replace( '\\', '/' ) \
                + NppComment.AutoUpdated( '#' )
        dataUpdated = re.sub( match, replace, dataUpdated )
        #---
        match   = r'(?P<NAME>CXXFLAGS)=\$\((?P=NAME)\)' \
                + r'(?P<FLAG>[ \t]+\$\(INCLUDEDIRS\))'
        replace = r'\g<NAME>=$(\g<NAME>)'       \
                + r' -DSCI_OWNREGEX'            \
                + r'\g<FLAG>'                   \
                + NppComment.AutoUpdated( '#' )
        dataUpdated = re.sub( match, replace, dataUpdated )
        #---
        match   = r'!IFDEF[ \t]+DEBUG(?P<EOL>\r?\n|\r)' \
                + r'(?P<DEBUG>(?:[^\r\n]+(?P=EOL))+)'   \
                + r'!ELSE(?P=EOL)'                      \
                + r'(?P<NDEBUG>(?:[^\r\n]+(?P=EOL))+)'  \
                + r'!ENDIF(?P=EOL)'
        replace = r'!IFDEF DEBUG\g<EOL>'                               \
                + r'\g<DEBUG>'                                         \
                + r'LDFLAGS=$(LDFLAGS) -LIBPATH:'                      \
                        + rootBoost.replace( '\\', '/' )               \
                        + r'/bin.v2/libs/regex/build/' + toolset       \
                        + r'/debug/link-static/runtime-link-static'    \
                                + r'/threading-multi'                  \
                        + NppComment.AutoGenerated( '#' ) + r'\g<EOL>' \
                + r'!ELSE\g<EOL>'                                      \
                + r'\g<NDEBUG>'                                        \
                + r'LDFLAGS=$(LDFLAGS) -LIBPATH:'                      \
                        + rootBoost.replace( '\\', '/' )               \
                        + r'/bin.v2/libs/regex/build/' + toolset       \
                        + r'/release/link-static/runtime-link-static'  \
                                + r'/threading-multi'                  \
                        + NppComment.AutoGenerated( '#' ) + r'\g<EOL>' \
                + r'!ENDIF\g<EOL>'
        dataUpdated = re.sub( match, replace, dataUpdated )
        #---
        match   = r'(?P<COMPRULES>'                                       \
                +   r'(?:'                                                \
                +     r'(?P<OBJDIR>\$\(\w+\)[\\/])\w+\.(?:OBJ|obj|Obj)'   \
                            + r'[^\r\n]+(?P<EOL>\r?\n|\r)'                \
                +     r'(?P<INDENT>[ \t]+)(?P<COMPNFLAGS>\$\(\w+\)[ \t]+' \
                            + r'\$\(\w+\))[^\r\n]+(?P=EOL)+'              \
                +   r')+'                                                 \
                + r')'
        replace = r'\g<COMPRULES>'                                        \
                + NppComment.AutoGeneratedBegin( '#' )                    \
                + r'\g<OBJDIR>UTF8DocumentIterator.obj:: '                \
                        + r'../src/UTF8DocumentIterator.cxx' + r'\g<EOL>' \
                + r'\g<INDENT>\g<COMPNFLAGS> -c '                         \
                        + r'../src/UTF8DocumentIterator.cxx' + r'\g<EOL>' \
                +                                              r'\g<EOL>' \
                + r'\g<OBJDIR>BoostRegexSearch.obj:: '                    \
                        + r'../src/BoostRegexSearch.cxx '                 \
                        + r'../src/CharClassify.h '                       \
                        + r'../src/RESearch.h'               + r'\g<EOL>' \
                + r'\g<INDENT>\g<COMPNFLAGS> -c '                         \
                        + r'../src/BoostRegexSearch.cxx'     + r'\g<EOL>' \
                + NppComment.AutoGeneratedEnd( '#' )                      \
                +                                              r'\g<EOL>'
        dataUpdated = re.sub( match, replace, dataUpdated )
        #---
        UpdateFile( path, dataUpdated )

#-------------------------------------------------------------------------------

def GetBoostVersion( rootBoost ):

    versionBoost = None

    dataRead = ReadFile( rootBoost + r'boost\version.hpp' )
    if dataRead != None:
        result = re.search( r'#define[ \t]+BOOST_LIB_VERSION[ \t]+'
                          + r'"(?P<VERSION>\d+(?:_\d+){1,2})"'
                          , dataRead )
        if result != None:
            versionBoost = result.group( 'VERSION' ).replace( '_', '.' )

    return versionBoost

#-------------------------------------------------------------------------------

def UpdateVersionInfo( rootScintilla, rootBoost = None ):

    versionBoost = None if rootBoost == None else GetBoostVersion( rootBoost )

    path     = rootScintilla + "win32\ScintRes.rc"
    dataRead = ReadFile( path )
    if dataRead != None:
        dataUpdated = dataRead
        #---
        match   = r'(?P<FLAGSMASK>FILEFLAGSMASK[ \t]+[^\r\n]+)(?P<EOL>\r?\n|\r)'
        replace = r'\g<FLAGSMASK> | VS_FF_SPECIALBUILD' \
                + NppComment.AutoUpdated()              \
                + r'\g<EOL>'
        dataUpdated = re.sub( match, replace, dataUpdated )
        #---
        match   = r'(?P<FLAGS>FILEFLAGS[ \t]+[^\r\n]+)(?P<EOL>\r?\n|\r)'
        replace = r'\g<FLAGS> | VS_FF_SPECIALBUILD' \
                + NppComment.AutoUpdated()          \
                + r'\g<EOL>'
        dataUpdated = re.sub( match, replace, dataUpdated )
        #---
        match    = r'(?P<PRODUCTVERSION>'           \
                 +   r'(?P<PRE>[ \t]+VALUE[ \t]+")' \
                 +   r'ProductVersion'              \
                 +   r'(?P<POST>",[ \t]+)'          \
                 +   r'[^\r\n]+(?P<EOL>\r?\n|\r)'   \
                 + r')'
        replace  = r'\g<PRODUCTVERSION>'           \
                 + NppComment.AutoGeneratedBegin() \
                 + r'\g<PRE>SpecialBuild\g<POST>'  \
                 + r'"for Notepad++'
        if versionBoost != None:
            replace += r' with Boost v' + versionBoost + r' RegEx Search'
        replace += r'\\0"\g<EOL>'               \
                 + NppComment.AutoGeneratedEnd()
        dataUpdated = re.sub( match, replace, dataUpdated )
        #---
        UpdateFile( path, dataUpdated )

#--===========================================================================--

def main( rootScintilla, rootBoost = None, toolset = None ):
    """
        [in] rootScintilla
        [in] rootBoost
        [in] toolset        e.g. "msvc-10.0"
    """
    if not os.path.exists( rootScintilla ):
        sys.exit( "ERROR: Can not find path `%s`" % rootScintilla )
    CorrectScintillaDotIFace( rootScintilla )

    AddNotifications( rootScintilla )
    AddMainWindowFocusOnMiddleButtonDown( rootScintilla )
    UpdateLexHTMLDotCxx( rootScintilla )

    # Add support for N++ specific lexers
    AddSearchResultLexer( rootScintilla )
    AddObjectiveCLexer( rootScintilla )
    AddUserDefinedLanguageLexer( rootScintilla )
    AddUniversalLexerStates( rootScintilla )

    #
    if rootBoost != None:
        if not os.path.exists( rootBoost ):
            print( "WARNING: Ignoring Boost, can not find path `%s`" % rootBoost )
        else:
            AddBoostRegExSearch( rootScintilla, rootBoost, toolset )

    # from Scintilla\scripts\LexGen import RegenerateAll
    cmd_subfolder = os.path.realpath( os.path.abspath( os.path.join(
          os.path.split( inspect.getfile( inspect.currentframe() ) )[0]
        , os.path.abspath( EnsureTrailingBackslash( rootScintilla + "scripts" ) )
        ) ) )
    if cmd_subfolder not in sys.path:
         sys.path.insert( 0, cmd_subfolder )
    from LexGen import RegenerateAll
    #
    RegenerateAll( rootScintilla )

    UpdateVersionInfo( rootScintilla, rootBoost )

#-=============================================================================-

if __name__ == "__main__":

    parser = argparse.ArgumentParser( prefix_chars = '-/' )
    parser.add_argument(
              '-s', '/s', '--ScintillaRoot'
            , help     = 'Root path of the Scintilla library to be converted'
            , type     = str
            , required = True
            , dest     = 'rootScintilla'
            , metavar  = 'path'
            )
    parser.add_argument(
              '-b', '/b', '--BoostRoot'
            , help     = 'Root path of the Boost library used'
            , type     = str
            , required = False
            , dest     = 'rootBoost'
            , metavar  = 'path'
            )
    parser.add_argument(
              '-t', '/t', '--toolset'
            , choices  = [ 'msvc-10.0', 'msvc-13.0', 'msvc-14.0' ]
            , default  = 'msvc-10.0'
            , help     = 'Toolset that was used to build the Boost library'
            , type     = str
            , required = False
            , dest     = 'toolset'
            , metavar  = 'toolset'
            )
    parser.add_argument(
              '-v', '/?', '--version'
            , action   = 'version'
            , version  = '%(prog)s v' + str( scriptVersion )
            )
    args = parser.parse_args()

    main( EnsureTrailingBackslash( args.rootScintilla )
        , EnsureTrailingBackslash( args.rootBoost )
        , args.toolset
        )

#-=============================================================================-
# End Of File
#-=============================================================================-
